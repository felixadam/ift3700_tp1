{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrique originale\n",
    "\n",
    "Une notion de distance définie comme la norme d'une transformation de (x + I) à (y + I) où I est la matrice identité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "    Mettre `skiprows=1` pour avoir toutes les données.\n",
    "    (Mais c'est très long surtout pour faire des tests.)\n",
    "\"\"\"\n",
    "mnist_train = np.loadtxt('mnist_train.csv', dtype='int', delimiter=',', skiprows=59001)\n",
    "mnist_test  = np.loadtxt('mnist_test.csv', dtype='int', delimiter=',', skiprows=9751)\n",
    "\n",
    "train_label = mnist_train[:,0]\n",
    "train_data  = mnist_train[:,1:]\n",
    "\n",
    "test_label  = mnist_test[:,0]\n",
    "test_data   = mnist_test[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ajout de la matrice identité peut être interprèter ici comme du bruit. Elle sert en fait à ce que les matrices aient un inverse pour pouvoir résoudre $ (B + I)x = Bx + Ix = y $. On cherche $ B $ qui est obtenue avec $ B = (y - x) x^{-1} $.\n",
    "\n",
    "La norme de la matrice est ici la plus grande valeur propre de la matrice $ B $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(28)\n",
    "\n",
    "def metric_4(X, Y):\n",
    "    x = X.reshape((28,28)) + I\n",
    "    y = Y.reshape((28,28)) + I\n",
    "    \n",
    "    eigv = np.linalg.eigvals((y - x) * np.linalg.inv(x))\n",
    "    \n",
    "    eigv = np.log([np.absolute(v) for v in eigv if not (v == 0)] + [1.0])\n",
    "    \n",
    "    return np.max(eigv)\n",
    "\n",
    "def metrics_4(exs):\n",
    "    s = exs.shape[0]\n",
    "    r = np.zeros((s,s))\n",
    "    for i in range(s):\n",
    "        for j in range(i, exs.shape[0]):\n",
    "            tmp = metric_4(exs[i], exs[j])\n",
    "            r[i,j] = tmp\n",
    "            r[j,i] = tmp\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "dis_matrix = metrics_4(train_data)\n",
    "print(dis_matrix.shape)\n",
    "#(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250)\n"
     ]
    }
   ],
   "source": [
    "test = metrics_4(test_data)\n",
    "print(test.shape)\n",
    "#(250, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À noter que `metric_4` n'est pas très performante et `metrics_4` à une complexité quadratique. J'ai essayé et abandonné avec 10000 exemples après ~1h00. Par contre, une fois la matrice de similarité calculé, les algorithmes ici-bas sont très rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de succes avec knn et la distance euclidienne: 83.2000 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "\n",
    "# `metric` peut être `callable` ici.\n",
    "algo = knn(n_neighbors=5, metric='euclidean', n_jobs=6)\n",
    "\n",
    "algo.fit(train_data, train_label)\n",
    "\n",
    "result = algo.score(test_data, test_label)\n",
    "\n",
    "print(\"Taux de succes avec knn et la distance euclidienne: %.4f %%\" % (result * 100.0))\n",
    "\n",
    "#Taux de succes avec knn et la distance euclidienne: 83.2000 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de succes avec knn et metric_4: 60.4000 %\n"
     ]
    }
   ],
   "source": [
    "# `metric` peut être `callable` ici.\n",
    "algo = knn(n_neighbors=5, metric=metric_4, n_jobs=6)\n",
    "\n",
    "algo.fit(train_data, train_label)\n",
    "\n",
    "result = algo.score(test_data, test_label)\n",
    "\n",
    "print(\"Taux de succes avec knn et metric_4: %.4f %%\" % (result * 100.0))\n",
    "\n",
    "#Taux de succes avec knn et metric_4: 60.4000 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note que c'est moins bon que la distance euclidienne mais le pure hasard serait de 10% alors que nous avons ici 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des prédictions (classes, prédictions):\n",
      "[[ 95.   0.   1.   0.   0.   0.   1.   0.   1.   0.]\n",
      " [102.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 95.   9.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [102.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 93.   1.   0.   0.   0.   1.   0.   1.   0.   0.]\n",
      " [ 87.   1.   0.   0.   1.   0.   0.   0.   0.   1.]\n",
      " [104.   0.   0.   1.   2.   0.   0.   0.   0.   0.]\n",
      " [103.   0.   3.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 93.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 98.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Argmax pour chaque classes:\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Dans la documentation ça semble être un classifieur qui utilise un arbre binaire\n",
    "# Il sera possible d'y accéder plus tard\n",
    "from sklearn.cluster import AgglomerativeClustering as BinaryTree\n",
    "\n",
    "# `affinity` peut être `callable` ici.\n",
    "# `linkage='average'` voir énoncé, dernier point de Conseils et indications\n",
    "algo = BinaryTree(n_clusters=10, linkage='average', affinity='precomputed')\n",
    "\n",
    "pred_labels = algo.fit_predict(dis_matrix)\n",
    "\n",
    "count_pred = np.zeros((10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(pred_labels.shape[0]):\n",
    "        if train_label[j] == i:\n",
    "            count_pred[i, pred_labels[j]] += 1\n",
    "\n",
    "print(\"Distribution des prédictions (classes, prédictions):\")\n",
    "print(count_pred)\n",
    "\n",
    "# Considérons la classe prédite la plus répéter comme la bonne classe\n",
    "# (Ne pouvons nous pas toujours faire ça?)\n",
    "argmax_pred = np.array([np.argmax(count_pred[i]) for i in range(10)])\n",
    "\n",
    "print(\"Argmax pour chaque classes:\")\n",
    "print(argmax_pred)\n",
    "\n",
    "#Distribution des prédictions (classes, prédictions):\n",
    "#[[ 95.   0.   1.   0.   0.   0.   1.   0.   1.   0.]\n",
    "# [102.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [ 95.   9.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [102.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [ 93.   1.   0.   0.   0.   1.   0.   1.   0.   0.]\n",
    "# [ 87.   1.   0.   0.   1.   0.   0.   0.   0.   1.]\n",
    "# [104.   0.   0.   1.   2.   0.   0.   0.   0.   0.]\n",
    "# [103.   0.   3.   0.   0.   0.   1.   0.   0.   0.]\n",
    "# [ 93.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [ 98.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
    "#Argmax pour chaque classes:\n",
    "#[0 0 0 0 0 0 0 0 0 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même classe est prédite à chaque fois. On pourrait donc parler de hasard et approximer cela à 10%.\n",
    "\n",
    "On note aussi quelque exemples qui selon l'algorithme ne ressemble à rien puisqu'ils sont seuls dans leur classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des prédictions (classes, prédictions):\n",
      "[[  0.   2.   3.   0.   3.  81.   2.   0.   7.   0.]\n",
      " [  0. 102.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  18.   0.   0.  78.   1.   0.   8.   0.   0.]\n",
      " [  0.  76.  12.   0.  15.   0.   0.   0.   0.   0.]\n",
      " [  1.  83.   0.   0.   0.   0.   0.   0.   0.  12.]\n",
      " [  0.  75.  15.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0. 107.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.  89.   0.  17.   0.   0.   1.   0.   0.   0.]\n",
      " [  1.  80.  12.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [  0.  86.   0.   0.   0.   0.   0.   0.   0.  12.]]\n",
      "Argmax pour chaque classes:\n",
      "[5 1 4 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Dans la documentation ça semble être un classifieur qui utilise un arbre binaire\n",
    "# Il sera possible d'y accéder plus tard\n",
    "from sklearn.cluster import AgglomerativeClustering as BinaryTree\n",
    "\n",
    "# `affinity` peut être `callable` ici.\n",
    "# `linkage='average'` voir énoncé, dernier point de Conseils et indications\n",
    "algo = BinaryTree(n_clusters=10, linkage='average', affinity='euclidean')\n",
    "\n",
    "pred_labels = algo.fit_predict(train_data)\n",
    "\n",
    "count_pred = np.zeros((10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(pred_labels.shape[0]):\n",
    "        if train_label[j] == i:\n",
    "            count_pred[i, pred_labels[j]] += 1\n",
    "\n",
    "print(\"Distribution des prédictions (classes, prédictions):\")\n",
    "print(count_pred)\n",
    "\n",
    "# Considérons la classe prédite la plus répéter comme la bonne classe\n",
    "# (Ne pouvons nous pas toujours faire ça?)\n",
    "argmax_pred = np.array([np.argmax(count_pred[i]) for i in range(10)])\n",
    "\n",
    "print(\"Argmax pour chaque classes:\")\n",
    "print(argmax_pred)\n",
    "\n",
    "#Distribution des prédictions (classes, prédictions):\n",
    "#[[  0.   2.   3.   0.   3.  81.   2.   0.   7.   0.]\n",
    "# [  0. 102.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [  0.  18.   0.   0.  78.   1.   0.   8.   0.   0.]\n",
    "# [  0.  76.  12.   0.  15.   0.   0.   0.   0.   0.]\n",
    "# [  1.  83.   0.   0.   0.   0.   0.   0.   0.  12.]\n",
    "# [  0.  75.  15.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [  0. 107.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
    "# [  0.  89.   0.  17.   0.   0.   1.   0.   0.   0.]\n",
    "# [  1.  80.  12.   0.   0.   1.   0.   0.   0.   0.]\n",
    "# [  0.  86.   0.   0.   0.   0.   0.   0.   0.  12.]]\n",
    "#Argmax pour chaque classes:\n",
    "#[5 1 4 1 1 1 1 1 1 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça n'est pas vraiment meilleur qu'avec la distance originale.\n",
    "\n",
    "Je me demande si les quelques exemples seuls dans leur classes ne seraient pas des observations abérantes du jeu de données. En d'autres mots, la distance varie énormément sur quelques données et très peu sur le reste.\n",
    "\n",
    "Est-ce que les données abérantes pourraient faussé les résultats d'un algorithme de regroupement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
